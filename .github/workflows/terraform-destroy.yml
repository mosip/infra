name: terraform destroy

on:
  workflow_dispatch:
    inputs:
      CLOUD_PROVIDER:
        description: 'Choose the cloud provider (aws, azure, gcp)'
        required: true
        type: choice
        options:
          - aws
          - azure
          - gcp
      TERRAFORM_COMPONENT:
        description: 'Choose component to destroy (WARNING: base-infra destruction removes foundational resources!)'
        required: true
        type: choice
        options:
          - infra
          - base-infra
          - observ-infra
        default: infra
      BACKEND_TYPE:
        description: 'Choose Terraform backend type (must match the one used during creation)'
        required: true
        type: choice
        options:
          - local
          - remote
        default: local
      REMOTE_BACKEND_CONFIG:
        description: 'Remote backend config (format: aws:bucket_base_name:region OR azure:rg_name:storage_account:container OR gcp:bucket_name). For AWS, bucket will be created as bucket_base_name-BRANCH_NAME'
        required: false
        type: string
      SSH_PRIVATE_KEY:
        description: 'The GitHub secret containing the private key of the SSH key named in the preceding input variable `SSH_PRIVATE_KEY` is used for SSH login purposes on nginx node.'
        required: true
      TERRAFORM_DESTROY:
        description: 'Terraform destroy - CONFIRM DESTRUCTION'
        required: false
        type: boolean
        default: false

env:
  # TF_LOG_PATH: ./temp/terraform.log
  ## TRACE, DEBUG, INFO, WARN or ERROR
  # TF_LOG: TRACE  
  # Repository-level secrets for cloud credentials
  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
  TF_VAR_ssh_private_key: ${{ secrets[inputs.SSH_PRIVATE_KEY] }}
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

jobs:
  terraform-destroy:
    runs-on: ubuntu-latest
    # Use dynamic environment based on current branch name
    environment: ${{ github.ref_name }}
    defaults:
      run:
        shell: bash
        # We work in the cloud-specific implementation directory
        working-directory: 'terraform/implementations/${{ inputs.CLOUD_PROVIDER }}/${{ inputs.TERRAFORM_COMPONENT }}'
    steps:
      - uses: actions/checkout@v4

      - name: Check for required implementation directory
        run: |
          if [ ! -d "." ]; then
            echo "Directory 'terraform/implementations/${{ inputs.CLOUD_PROVIDER }}/${{ inputs.TERRAFORM_COMPONENT }}' does not exist."
            echo "Available providers: aws, azure, gcp"
            echo "Available components: base-infra, infra"
            exit 1
          fi
          
          # Check if tfvars file exists
          if [ ! -f "${{ inputs.CLOUD_PROVIDER }}.tfvars" ]; then
            echo "Configuration file '${{ inputs.CLOUD_PROVIDER }}.tfvars' not found in current directory."
            exit 1
          fi

      - name: Setup S3 Bucket for Remote State (AWS only)
        run: |
          if [ "${{ inputs.BACKEND_TYPE }}" = "remote" ] && [ "${{ inputs.CLOUD_PROVIDER }}" = "aws" ]; then
            # Extract bucket name from REMOTE_BACKEND_CONFIG or create dynamic name
            BACKEND_CONFIG="${{ inputs.REMOTE_BACKEND_CONFIG }}"
            BRANCH_NAME="${{ github.ref_name }}"
            
            if [ -n "$BACKEND_CONFIG" ]; then
              # Parse provided config: aws:bucket_name:region
              IFS=':' read -ra CONFIG_PARTS <<< "$BACKEND_CONFIG"
              BUCKET_BASE_NAME="${CONFIG_PARTS[1]}"
              REGION="${CONFIG_PARTS[2]:-us-east-1}"
            else
              # Create dynamic bucket name if not provided
              BUCKET_BASE_NAME="mosip-terraform-state"
              REGION="us-east-1"
            fi
            
            # Create dynamic bucket name with branch (same bucket for all components - recommended)
            DYNAMIC_BUCKET_NAME="${BUCKET_BASE_NAME}-${BRANCH_NAME}"
            
            echo "🪣 Connecting to S3 bucket for Terraform state..."
            echo "Base bucket name: $BUCKET_BASE_NAME"
            echo "Branch name: $BRANCH_NAME"
            echo "Dynamic bucket name: $DYNAMIC_BUCKET_NAME"
            echo "Region: $REGION"
            
            # Check if bucket exists (required for destroy)
            if aws s3api head-bucket --bucket "$DYNAMIC_BUCKET_NAME" 2>/dev/null; then
              echo "✅ Bucket $DYNAMIC_BUCKET_NAME found - ready for destroy"
            else
              echo "⚠️  Warning: Bucket $DYNAMIC_BUCKET_NAME not found"
              echo "   This may be normal if resources were already destroyed or created with local backend"
            fi
            
            # Store dynamic bucket name for next step
            echo "DYNAMIC_BUCKET_NAME=$DYNAMIC_BUCKET_NAME" >> $GITHUB_ENV
            echo "DYNAMIC_REGION=$REGION" >> $GITHUB_ENV
            
            echo "🎯 S3 bucket check completed: $DYNAMIC_BUCKET_NAME"
          else
            echo "ℹ️  Skipping S3 bucket setup (not AWS remote backend)"
          fi

      - name: Configure Terraform Backend for State Management
        run: |
          # Configure backend based on user choice and cloud provider
          echo "🔧 Configuring Terraform backend..."
          echo "Backend type: ${{ inputs.BACKEND_TYPE }}"
          echo "Cloud provider: ${{ inputs.CLOUD_PROVIDER }}"
          echo "Component: ${{ inputs.TERRAFORM_COMPONENT }}"
          
          if [ "${{ inputs.BACKEND_TYPE }}" = "local" ]; then
            echo "📁 Using local backend - state files will be stored locally and committed to git"
            echo "State file will be: ${{ inputs.CLOUD_PROVIDER }}-${{ inputs.TERRAFORM_COMPONENT }}-terraform.tfstate"
            
            # Create terraform configuration with local backend
            cat > backend.tf << EOF
          terraform {
            backend "local" {
              path = "${{ inputs.CLOUD_PROVIDER }}-${{ inputs.TERRAFORM_COMPONENT }}-terraform.tfstate"
            }
          }
          EOF
            
            echo "✅ Local backend configuration created"
            
          elif [ "${{ inputs.BACKEND_TYPE }}" = "remote" ]; then
            echo "☁️  Using remote backend for state management"
            
            # Parse REMOTE_BACKEND_CONFIG
            BACKEND_CONFIG="${{ inputs.REMOTE_BACKEND_CONFIG }}"
            if [ -z "$BACKEND_CONFIG" ]; then
              echo "❌ ERROR: REMOTE_BACKEND_CONFIG is required for remote backend"
              echo "Format: aws:bucket_name:region OR azure:rg_name:storage_account:container OR gcp:bucket_name"
              exit 1
            fi
            
            IFS=':' read -ra CONFIG_PARTS <<< "$BACKEND_CONFIG"
            PROVIDER_TYPE="${CONFIG_PARTS[0]}"
            
            if [ "${{ inputs.CLOUD_PROVIDER }}" = "aws" ] && [ "$PROVIDER_TYPE" = "aws" ]; then
              # Use dynamic bucket name from previous step or parse from config
              if [ -n "$DYNAMIC_BUCKET_NAME" ]; then
                BUCKET_NAME="$DYNAMIC_BUCKET_NAME"
                REGION="$DYNAMIC_REGION"
                echo "Using dynamically created bucket: $BUCKET_NAME"
              else
                # Fallback to parsing from REMOTE_BACKEND_CONFIG
                BUCKET_NAME="${CONFIG_PARTS[1]}"
                REGION="${CONFIG_PARTS[2]:-us-east-1}"
              fi
              
              if [ -z "$BUCKET_NAME" ]; then
                echo "❌ ERROR: Bucket name is required for AWS S3 backend"
                echo "Format: aws:bucket_name:region"
                exit 1
              fi
              
              # Create branch-specific key (must match the one used during creation)
              BRANCH_NAME="${{ github.ref_name }}"
              STATE_KEY="${{ inputs.CLOUD_PROVIDER }}-${{ inputs.TERRAFORM_COMPONENT }}-${BRANCH_NAME}-terraform.tfstate"
              
              echo "Configuring AWS S3 backend..."
              echo "Bucket: $BUCKET_NAME"
              echo "Region: $REGION"
              echo "Branch: $BRANCH_NAME"
              echo "State Key: $STATE_KEY"
              
              cat > backend.tf << EOF
          terraform {
            backend "s3" {
              bucket = "$BUCKET_NAME"
              key    = "$STATE_KEY"
              region = "$REGION"
            }
          }
          EOF
              
              echo "✅ AWS S3 backend configuration created"
              
            elif [ "${{ inputs.CLOUD_PROVIDER }}" = "azure" ] && [ "$PROVIDER_TYPE" = "azure" ]; then
              RESOURCE_GROUP="${CONFIG_PARTS[1]}"
              STORAGE_ACCOUNT="${CONFIG_PARTS[2]}"
              CONTAINER="${CONFIG_PARTS[3]:-terraform-state}"
              
              if [ -z "$RESOURCE_GROUP" ] || [ -z "$STORAGE_ACCOUNT" ]; then
                echo "❌ ERROR: Resource Group and Storage Account are required for Azure backend"
                echo "Format: azure:rg_name:storage_account:container"
                exit 1
              fi
              
              echo "Configuring Azure Storage backend..."
              echo "Resource Group: $RESOURCE_GROUP"
              echo "Storage Account: $STORAGE_ACCOUNT"
              echo "Container: $CONTAINER"
              
              cat > backend.tf << EOF
          terraform {
            backend "azurerm" {
              resource_group_name  = "$RESOURCE_GROUP"
              storage_account_name = "$STORAGE_ACCOUNT"
              container_name       = "$CONTAINER"
              key                  = "${{ inputs.CLOUD_PROVIDER }}-${{ inputs.TERRAFORM_COMPONENT }}-terraform.tfstate"
            }
          }
          EOF
              
              echo "✅ Azure Storage backend configuration created"
              
            elif [ "${{ inputs.CLOUD_PROVIDER }}" = "gcp" ] && [ "$PROVIDER_TYPE" = "gcp" ]; then
              BUCKET_NAME="${CONFIG_PARTS[1]}"
              
              if [ -z "$BUCKET_NAME" ]; then
                echo "❌ ERROR: Bucket name is required for GCS backend"
                echo "Format: gcp:bucket_name"
                exit 1
              fi
              
              echo "Configuring GCS backend..."
              echo "Bucket: $BUCKET_NAME"
              
              cat > backend.tf << EOF
          terraform {
            backend "gcs" {
              bucket = "$BUCKET_NAME"
              prefix = "terraform/${{ inputs.CLOUD_PROVIDER }}-${{ inputs.TERRAFORM_COMPONENT }}"
            }
          }
          EOF
              
              echo "✅ GCS backend configuration created"
            else
              echo "❌ ERROR: Backend configuration mismatch"
              echo "Cloud provider: ${{ inputs.CLOUD_PROVIDER }}"
              echo "Backend config provider: $PROVIDER_TYPE"
              echo "They must match!"
              exit 1
            fi
          fi
          
          echo "📋 Backend configuration:"
          cat backend.tf

      - name: Skip SSH Host key verification
        run: |
          mkdir -p ~/.ssh
          echo -e "Host *\n     StrictHostKeyChecking no" > ~/.ssh/config

      - name: Setup ufw firewall
        run: |
          sudo ufw enable
          sudo ufw allow ssh
          sudo ufw allow 443/tcp
          sudo ufw allow 51820/udp
          sudo ufw status
        if: "${{ inputs.TERRAFORM_COMPONENT != 'base-infra' }}"

      - name: Install WireGuard
        run: sudo apt-get install -y wireguard
        if: "${{ inputs.TERRAFORM_COMPONENT != 'base-infra' }}"

      - name: Configure WireGuard
        run: |
          # WG_CONFIG comes from environment-level secrets (dynamic based on branch name)
          echo "${{ secrets.TF_WG_CONFIG }}" | sudo tee /etc/wireguard/wg0.conf
        if: "${{ inputs.TERRAFORM_COMPONENT != 'base-infra' }}"

      - name: Start WireGuard
        run: |
          sudo chmod 600 /etc/wireguard/wg0.conf
          sudo chmod 700 /etc/wireguard/
          sudo chmod 644 /lib/systemd/system/wg-quick@.service
          sudo systemctl daemon-reload
          sudo wg-quick up wg0
          sudo wg show wg0
        if: "${{ inputs.TERRAFORM_COMPONENT != 'base-infra' }}"

      - name: Setup Terraform with specified version on the runner
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: v1.8.5

      - name: Display Destruction Warning
        run: |
          echo "🚨 DESTRUCTIVE OPERATION: ${{ inputs.CLOUD_PROVIDER }} (${{ inputs.TERRAFORM_COMPONENT }})"
          echo "📁 Working directory: $(pwd)"
          echo "📋 Available files:"
          ls -la
          
          if [ "${{ inputs.TERRAFORM_COMPONENT }}" = "base-infra" ]; then
            echo "💥 CRITICAL WARNING: base-infra destruction will remove foundational resources!"
            echo "   This includes VPCs, subnets, routing tables, security groups, etc."
            echo "   This is typically a ONE-TIME setup and should rarely be destroyed."
            echo "   Make sure you understand the impact before proceeding."
          else
            echo "ℹ️  INFO: infra destruction - removes MOSIP application infrastructure"
            echo "   This is safe to destroy and recreate as needed."
          fi
          
          if [ "${{ inputs.TERRAFORM_DESTROY }}" != "true" ]; then
            echo "❌ TERRAFORM_DESTROY is not set to true - skipping actual destruction"
          else
            echo "✅ TERRAFORM_DESTROY confirmed - proceeding with destruction"
          fi

      - name: Terraform Init
        run: terraform init

      - name: Terraform refresh
        run: terraform refresh -var-file="${{ inputs.CLOUD_PROVIDER }}.tfvars" -no-color
        if: "${{ inputs.TERRAFORM_DESTROY  == true }}"

      - name: Terraform Destroy Plan
        id: destroy-plan
        run: |
          echo "📋 Showing what will be destroyed..."
          terraform plan -destroy -var-file="${{ inputs.CLOUD_PROVIDER }}.tfvars" -no-color
        if: "${{ inputs.TERRAFORM_DESTROY  == true }}"
        continue-on-error: true

      - name: Terraform Destroy
        id: destroy
        run: terraform destroy -var-file="${{ inputs.CLOUD_PROVIDER }}.tfvars" -no-color -auto-approve
        if: "${{ inputs.TERRAFORM_DESTROY  == true }}"
        continue-on-error: true

      - name: Clean up state files and artifacts
        run: |
          echo "🧹 Cleaning up local state files and artifacts..."
          rm -f terraform.tfstate*
          rm -f backend.tf
          echo "✅ Cleanup completed"
        if: "${{ inputs.TERRAFORM_DESTROY == true && steps.destroy.outcome == 'success' }}"

      - name: Add the Terraform state changes
        run: |
          echo "📁 Current working directory: $(pwd)"
          echo "📋 Files in current directory:"
          ls -la
          
          echo "🔍 Checking for Terraform files to commit..."
          
          # Force add Terraform files even if they're in .gitignore
          git add -f *.tfstate* 2>/dev/null || echo "No .tfstate files found"
          git add -f backend.tf 2>/dev/null || echo "No backend.tf file found"
          git add -f *.tf 2>/dev/null || echo "No .tf files to add"
          # Also add any other Terraform-related files that might have been created
          git add -A
          
          echo "📊 Git status after adding files:"
          git status
          
          # Check if there are any changes to commit
          if git diff --cached --quiet; then
            echo "ℹ️  No changes to commit - this is normal for remote backends or when state is managed remotely"
            echo "✅ Terraform destroy completed successfully"
          else
            echo "📝 Committing Terraform changes..."
            git config --global user.email ${{ github.actor }}@users.noreply.github.com
            git config --global user.name ${{ github.actor }}
            git commit -s -am "Terraform destroy GitHub Actions - ${{ inputs.CLOUD_PROVIDER }}-${{ inputs.TERRAFORM_COMPONENT }}"
            git push
            echo "✅ Changes committed and pushed successfully"
          fi
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        if: "${{ inputs.TERRAFORM_DESTROY == true }}"
      - name: Terraform destroy status
        if: ${{ steps.destroy.outcome == 'failure' }}
        run: |
          echo "❌ Terraform destroy failed"
          echo "Check the logs above for detailed error information"
          exit 1

      - name: Terraform destroy success
        if: ${{ steps.destroy.outcome == 'success' }}
        run: |
          echo "✅ Terraform destroy completed successfully"
          echo "🗑️  All ${{ inputs.CLOUD_PROVIDER }}-${{ inputs.TERRAFORM_COMPONENT }} resources have been destroyed"

      - uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          fields: repo,message,author,commit,workflow,job # selectable (default: repo,message)
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }} # required
        if: "${{ github.event_name != 'pull_request' && failure() }}" # Pick up events even if the job fails or is canceled.
